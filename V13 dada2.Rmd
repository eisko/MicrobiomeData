---
title: "V1-3 Dada2 Processing"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("dada2")
library("ggplot2")
library("tidyr")
library("dplyr")
library("stringr")

```

## Dada2 Pipeline

See [dada2 github](https://benjjneb.github.io/dada2/index.html) for more details including tutorial (used to construct this pipeline).

Note, loaded dada2 from anaconda cloud, v1.14.0

Assumptions made in making this pipeline:
* samples were already demultiplexed, (true, cuz reads split into individual per-sample fastq files)
* adapters were removed 
* and sequences remain unpaired (forward and reverse reads seperate)

### Define paths

```{r}
# create list of sample locations
path <- "/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse"
samples <- sort(list.files(path, pattern = ".fastq.gz", full.names = TRUE))
samples

```


```{r}
# extract forward and reverse fastq filenames
fnFs <- sort(list.files(path, pattern = ".R1.catrim.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = ".R2.catrim.fastq.gz", full.names = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME.XXX.fastq.gz
sample.names <- sapply(strsplit(basename(fnFs), ".R1"), `[`, 1)
sample.names
```



### Inspect read quality profiles
```{r}
# inspect quality for forward reads
plotQualityProfile(fnFs[1:2])
```

```{r}
# inspect quality for reverse reads
plotQualityProfile(fnRs[1:2])
```



### Filter and trim


```{r}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```


```{r}
# takes awhile - not sure, maybe 10 mins?
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs,
                      maxN=0, maxEE=c(2,4), truncQ=2, rm.phix=TRUE,
                      compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
head(out)
```

# inspect quality of filtered data

```{r}
plotQualityProfile(filtFs[1:2])
plotQualityProfile(filtRs[1:2])
```

### Learn error rates
```{r}
# takes about 10 mins to run
errF <- learnErrors(filtFs)
errR <- learnErrors(filtRs)
```

### inspect errors
```{r}
plotErrors(errR, nominalQ = TRUE)
```



### save progress, so don't have to rerun
```{r}
saveRDS(filtFs, "/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/filtFs.rds")
saveRDS(errF, "/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/errF.rds")
saveRDS(filtRs, "/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/filtRs.rds")
saveRDS(errR, "/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/errR.rds")
```


## START HERE - LOAD IN PROGRESS
```{r}
filtFs <- readRDS("/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/filtFs.rds")
errF <- readRDS("/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/errF.rds")
filtRs <- readRDS("/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/filtRs.rds")
errR <- readRDS("/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/errR.rds")
```



### (Re)Inspect error
```{r}
plotErrors(errF, nominalQ = TRUE)
plotErrors(errR, nominalQ = TRUE)
```

Errors look good. Errors decrease as quality increases (as expected). Looks similar to tutorial (see online tutorial). Red = predicted error from Q score, black line = learned error rate from reads that will be used for sample inference/dada algoritmh

## Sample Inference

Following is the main/core dada algorithm:
```{r}
dadaFs <- dada(filtFs, err=errF, multithread = TRUE)
dadaRs <- dada(filtRs, err = errR, multithread = TRUE)
```


```{r}
# inspect element
dadaFs[[1]]
# to see more to dada-class return object see `help("dada-class")`
# can pool samples - don't think would want to? or use for low quality/low abundance samples?
```

### save progress
```{r}
saveRDS(dadaFs, "/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/dadaFs.RDS")
saveRDS(dadaRs, "/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/dadaRs.RDS")

```

### NOW START HERE - load in dada objects
```{r}
dadaFs <- readRDS("/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/dadaFs.RDS")
dadaRs <- readRDS("/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/dadaRs.RDS")
```



## Merge paired reads
- reverse/forward denoised reads only merged into 'contig' sequences if forward and reverse reads overlap by at least 12 bases and are identical to each other in the overlap region
```{r}
#used defaults settings for mergePairs: minOverlap = 12, maxMismatch = 0 (no mismatches allowed)
# Sean also uses minOverlap=10, maxMismatch = 1, which is less strict but may results in ASVs with indels
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose = TRUE)
head(mergers[[1]])
```


### START HERE!!!!! ####
```{r}
# save progress
saveRDS(mergers, file = "/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/mergers.RDS")
```

```{r}
# read in mergers files
mergers <- readRDS("/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/mergers.RDS")
```




## Construct sequence (ASV) table

The sequence table is a matrix, 
rows = samples
columns = seq variants (as in ACTG sequencs of ASV)
```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```
# this table contains 9176 ASVs and 96 samples???


```{r}
#Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))

seqlengths <- table(nchar(getSequences(seqtab)))
```
## Remove Chimeras
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method = "consensus", multithread = TRUE, verbose=TRUE)
dim(seqtab.nochim)

```

### START HERE!!!###
```{r}
saveRDS(seqtab.nochim, file="/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/seqtab.nochim.RDS")
```
```{r}
seqtab.nochim <- readRDS("/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/seqtab.nochim.RDS")
```



Seems like majority are identified as bimera sequences, better to be more strict during mergePairs???
- according to dada2 tutorial, not uncommon that a majority of sequence variants are to be removed after this step
```{r}
sum(seqtab.nochim)/sum(seqtab)
```
Retained majority of reads - so will continue on with tutorial

## Track reads through the pieline
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single samp;le, remove the sapply calls: e.g. replace sapply(dadaFs, getN), with getN(dadaFs)

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

```{r}
# look at same table but in terms of percentage of input
track_percent <- track/track[,1]*100
head(track_percent)
```
What is shown in tutorial:
Raw:
SampleNames input filtered denoisedF denoisedR merged nonchim
F3D0    7793     7113      6976      6979   6540    6528
F3D1    5869     5299      5227      5239   5028    5017
F3D141  5958     5463      5331      5357   4986    4863
F3D142  3183     2914      2799      2830   2595    2521
F3D143  3178     2941      2822      2867   2552    2518
F3D144  4827     4312      4151      4228   3627    3488

Percent:
input filtered denoisedF denoisedR   merged  nonchim
1   100 91.27422  89.51623  89.55473 83.92147 83.76748
2   100 90.28795  89.06117  89.26563 85.67047 85.48305
3   100 91.69184  89.47633  89.91272 83.68580 81.62135
4   100 91.54885  87.93591  88.90983 81.52686 79.20201
5   100 92.54248  88.79799  90.21397 80.30208 79.23222
6   100 89.33085  85.99544  87.59064 75.13984 72.26020



## Assign taxonomy
Uses machine learning to assign taxonomy. Learns on reference training set and uses naive Bayesian classifier method to assign taxonomy of unknown samples/sequences based on reference

**This is where reference used could matter!!!**

Dada2 maintains formatted training fastas for this set
https://benjjneb.github.io/dada2/training.html
Using Silva version 132 for the following
Start: 4:32 pm, end:4:52
```{r}
taxa <- assignTaxonomy(seqtab.nochim, "~/code/Stool/silva_nr_v132_train_set.fa", multithread = TRUE, tryRC=TRUE)
```
... not sure how long this step took, at least 15 mins, maybe more
can also add extension of spcies level assignment
taxa <- addSpecies(taxa, "~/code/Stool/silva_species_assignment_v132.fa.gz")

### START HERE!!! ###
```{r}
saveRDS(taxa, file = "/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/taxa.RDS")
```
```{r}
taxa <- readRDS("/Users/iskoec/code/Stool/MS0067_MMAMicro/mouse/taxa.RDS")
```


```{r}
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```

### Assign taxa using DECIPHER

Tutorial also sets up alternative using DECIPHER or IDTAXA algorithm, which should perform better than the naive Bayesian classifier:

(need to download alternative format database to run)
```{r}
library(DECIPHER)
```
```{r}

```

## PCA on samples using seqtable
```{r}
pca <- prcomp(seqtab.nochim)
```

